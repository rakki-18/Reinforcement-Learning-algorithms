{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the environment\n",
    "number_of_states = 16\n",
    "number_of_actions = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = np.zeros((16,4),np.int)\n",
    "terminal_state = np.zeros((16,4))\n",
    "reward = np.zeros((16,4))\n",
    "\n",
    "# creating a transition probability matrix next_state[s][a]which stores the next state it goes to after completing the action,\n",
    "#reward[s][a] stores the reward  it gets\n",
    "# and terminal_state[s][a] whether it reaches the terminal step or not\n",
    "for i in range(4):\n",
    "    next_state[0][i]= 0\n",
    "    reward[0][i]=0\n",
    "    terminal_state[0][i]=True\n",
    "for i in range(4):\n",
    "    next_state[15][i]= 15\n",
    "    reward[15][i]=0\n",
    "    terminal_state[15][i]=True\n",
    "for i in range(1,15):\n",
    "    for j in range(4):\n",
    "        reward[i][j] = -1\n",
    "for i in range(1,15):\n",
    "    if(i-4>=0):\n",
    "        next_state[i][0]= i-4\n",
    "    else:\n",
    "        next_state[i][0] = i\n",
    "    if(next_state[i][0]==0 or next_state[i][0]==15):\n",
    "        terminal_state[i][0]= True\n",
    "        \n",
    "    else:\n",
    "        terminal_state[i][0]=False\n",
    "for i in range(1,15):\n",
    "    if(i%4!=0):\n",
    "        next_state[i][1]= i-1\n",
    "    else:\n",
    "        next_state[i][1] = i\n",
    "    if(next_state[i][1]==0 or next_state[i][1]==15):\n",
    "        terminal_state[i][1]= True\n",
    "    else:\n",
    "        terminal_state[i][1]=False\n",
    "for i in range(1,15):\n",
    "    if(i%4!=3):\n",
    "        next_state[i][2]= i+1\n",
    "    else:\n",
    "        next_state[i][2] = i\n",
    "    if(next_state[i][2]==0 or next_state[i][2]==15):\n",
    "        terminal_state[i][2]= True\n",
    "    else:\n",
    "        terminal_state[i][2]=False\n",
    "for i in range(1,15):\n",
    "    if(i+4<=15):\n",
    "        next_state[i][3]= i+4\n",
    "    else:\n",
    "        next_state[i][3] = i\n",
    "    if(next_state[i][3]==0 or next_state[i][3]==15):\n",
    "        terminal_state[i][3]= True\n",
    "    else:\n",
    "        terminal_state[i][3]=False\n",
    "rewards = np.zeros((16,1))\n",
    "for i in range(16):\n",
    "    if(i==0 or i==15):\n",
    "        rewards[i] = 0\n",
    "    else :\n",
    "        rewards[i]= -1\n",
    "# creating the environment completed      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy evaluation\n",
    "def policy_evaluation(policy, next_state, reward ):\n",
    "    # value function\n",
    "    discount_factor=0.999\n",
    "    theta = 0.00000001\n",
    "    V = np.zeros(16)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        #finding value function for every state\n",
    "        for s in range(int(16)):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for i in range(int(4)):\n",
    "                # For each action, look at the possible next states\n",
    "                 v += policy[s][i] *1* (reward[s][i]+ discount_factor * V[next_state[s][i]])\n",
    "            # seeing how much our value function changed \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a random policy for every action of each state with equal probability\n",
    "policy = np.ones([16,4])/4\n",
    "v = policy_evaluation(policy , next_state, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function\n",
      "[[  0.         -13.76222671 -19.64826244 -21.60700716]\n",
      " [-13.76222671 -17.68951773 -19.65022119 -19.64826245]\n",
      " [-19.64826244 -19.65022119 -17.68951773 -13.76222672]\n",
      " [-21.60700716 -19.64826245 -13.76222672   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#printing the final value function\n",
    "print(\"value function\")\n",
    "print(v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing policy iteration\n",
    "def policy_iteration(next_state, reward):\n",
    "    discount_factor = 0.99\n",
    "    # finding the q values of every action corresponding to a given state \n",
    "    def q_value(state, V):\n",
    "        A = np.zeros([4,1])\n",
    "     \n",
    "        for a in range(4):\n",
    "            A[a] = 1* (reward[state][a] + discount_factor* V[next_state[state][a]])\n",
    "        return A\n",
    "     # making  a random policy\n",
    "    policy = np.ones([16,4])/4\n",
    "    while True:\n",
    "        # evaluating the current policy by our previous function\n",
    "        V = policy_evaluation(policy, next_state , reward)\n",
    "       \n",
    "        optimal_policy = True\n",
    "        for s in range(16):\n",
    "            # finding which action will be taken according to the current policy\n",
    "            chosen_action = np.argmax(policy[s])\n",
    "            # finding q values corresponding to each action in the given state\n",
    "            q_values = q_value(s,V)\n",
    "            best_action = np.argmax(q_values)\n",
    "            \n",
    "            # making the policy greedy by taking the best action\n",
    "            if(best_action!=chosen_action):\n",
    "                optimal_policy = False\n",
    "            policy[s] = np.eye(1,4, best_action)\n",
    "            \n",
    "        # if we have found the optimal policy, stop iterating    \n",
    "        if(optimal_policy):\n",
    "            return policy,V\n",
    "          \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function\n",
      "[[ 0.       -1.       -1.999    -2.997001]\n",
      " [-1.       -1.999    -2.997001 -1.999   ]\n",
      " [-1.999    -2.997001 -1.999    -1.      ]\n",
      " [-2.997001 -1.999    -1.        0.      ]]\n",
      "policy function for each state where 0-> go up 1-> go left 2-> go right 3-> go down \n",
      "[[0 1 1 1]\n",
      " [0 0 0 3]\n",
      " [0 0 2 3]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_iteration(next_state, reward)\n",
    "print(\"value function\")\n",
    "print(v.reshape(4,4))\n",
    "\n",
    "print(\"policy function for each state where 0-> go up 1-> go left 2-> go right 3-> go down \")\n",
    "print(np.reshape(np.argmax(policy, axis=1), (4,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(next_state, reward):\n",
    "    theta = 0.0001\n",
    "    discount_factor = 0.99\n",
    "    # initialising value function to zero\n",
    "    \n",
    "    V = np.zeros((16,1))\n",
    "\n",
    "    while True:\n",
    "            # usd for terminating condition\n",
    "            delta = 0\n",
    "            # going through all the states\n",
    "            for s in range(16):\n",
    "                # finding q_value for each action in the state\n",
    "                A = np.zeros((4,1))\n",
    "                for a in range(4):\n",
    "                    A[a] = np.max(reward[s][a] + discount_factor * V[next_state[s][a]])\n",
    "                # choosing the best action\n",
    "                best_action = np.max(A)\n",
    "                # finding how much the value function changes\n",
    "                delta = max(delta, np.abs(best_action - V[s]))\n",
    "                # updating the value function for that state\n",
    "                V[s] = best_action\n",
    "            # checking if the value function has started converging\n",
    "            if(delta<theta):\n",
    "                 break\n",
    "    # finding the optimal policy for each state\n",
    "    policy = np.zeros((16,4))\n",
    "    for s in range(16):\n",
    "        A = np.zeros((4,1))\n",
    "        for a in range(4):\n",
    "            A[a]= reward[s][a] + discount_factor*V[next_state[s][a]]\n",
    "        # choosing the action which has  the highest q_value\n",
    "        best_action = np.argmax(A)\n",
    "        # always selecting that action for that state\n",
    "        policy[s][best_action] = 1\n",
    "    return policy , V         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy , v = value_iteration(next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function\n",
      "[[ 0.     -1.     -1.99   -2.9701]\n",
      " [-1.     -1.99   -2.9701 -1.99  ]\n",
      " [-1.99   -2.9701 -1.99   -1.    ]\n",
      " [-2.9701 -1.99   -1.      0.    ]]\n",
      "policy function for each state where 0-> go up 1-> go left 2-> go right 3-> go down \n",
      "[[0 1 1 1]\n",
      " [0 0 0 3]\n",
      " [0 0 2 3]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"value function\")\n",
    "print(v.reshape(4,4))\n",
    "print(\"policy function for each state where 0-> go up 1-> go left 2-> go right 3-> go down \")\n",
    "print(np.reshape(np.argmax(policy, axis=1), (4,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
